{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/article/4586757-research-frontiers-inc-refr-q4-2022-earnings-call-transcript\n",
      "attempting to grab page: https://seekingalpha.com//article/4586757-research-frontiers-inc-refr-q4-2022-earnings-call-transcript\n",
      "refr_mar. 11, 2023 10:43 am  sucessfully saved\n",
      "/article/4586674-rlx-technology-inc-rlx-q4-2022-earnings-call-transcript\n",
      "attempting to grab page: https://seekingalpha.com//article/4586674-rlx-technology-inc-rlx-q4-2022-earnings-call-transcript\n",
      "rlx_mar. 10, 2023 9:26 pm  sucessfully saved\n",
      "/article/4586673-docebo-inc-dcbo-q4-2022-earnings-call-transcript\n",
      "attempting to grab page: https://seekingalpha.com//article/4586673-docebo-inc-dcbo-q4-2022-earnings-call-transcript\n",
      "dcbo_mar. 10, 2023 8:08 pm  sucessfully saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m.5\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m200\u001b[39m): \u001b[39m#choose what pages of earnings to scrape\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     process_list_page(i)\n",
      "Cell \u001b[0;32mIn[14], line 49\u001b[0m, in \u001b[0;36mprocess_list_page\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     47\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://seekingalpha.com/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m url_ending\n\u001b[1;32m     48\u001b[0m grab_page(url)\n\u001b[0;32m---> 49\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m.5\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_date(c):\n",
    "    end = c.find('|')\n",
    "    return c[0:end-1]\n",
    "\n",
    "def get_ticker(c):\n",
    "    beg = c.find('(')\n",
    "    end = c.find(')')\n",
    "    return c[beg+1:end]\n",
    "\n",
    "def grab_page(url):\n",
    "    print(\"attempting to grab page: \" + url)\n",
    "    page = requests.get(url)\n",
    "    page_html = page.text\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "    nameMeta = soup.find(\"a\",{'class':'xs-YW'})\n",
    "    dateText = soup.find(\"span\",{'data-test-id':'post-date'})\n",
    "    content = soup.find('div',{'class' : 'dB-d'})\n",
    "\n",
    "    if type(nameMeta) and type(content) and type(dateText) == \"NoneType\":\n",
    "        print(\"skipping this link, no content here\")\n",
    "        return\n",
    "    else:\n",
    "        text = content.text\n",
    "        name = nameMeta.text\n",
    "        date = dateText.text\n",
    "\n",
    "        filename = get_ticker(name) + \"_\" + get_date(date)\n",
    "        file = open(filename.lower() + \".txt\", 'w')\n",
    "        file.write(text)\n",
    "        file.close\n",
    "        print(filename.lower()+ \" sucessfully saved\")\n",
    "\n",
    "def process_list_page(i):\n",
    "    origin_page = \"https://seekingalpha.com/earnings/earnings-call-transcripts\"\n",
    "    page = requests.get(origin_page)\n",
    "    page_html = page.text\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    alist = soup.find_all(\"a\",{'data-test-id':'post-list-item-title'})\n",
    "    for i in range(0,len(alist)):\n",
    "        url_ending = alist[i].attrs['href']\n",
    "        print(url_ending)\n",
    "        url = \"https://seekingalpha.com/\" + url_ending\n",
    "        grab_page(url)\n",
    "        time.sleep(.5)\n",
    "\n",
    "for i in range(1,200): #choose what pages of earnings to scrape\n",
    "    process_list_page(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da7d2da60040266494a0835303b76fb07a5af0751b754573e516b24e42d20d6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
